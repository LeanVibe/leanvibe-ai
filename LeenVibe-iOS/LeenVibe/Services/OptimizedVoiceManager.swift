import SwiftUI
import AVFoundation
import Speech
import Combine

@MainActor
class OptimizedVoiceManager: ObservableObject {
    @Published var isOptimized = false
    @Published var responseTime: TimeInterval = 0
    @Published var currentLatency: TimeInterval = 0
    @Published var performanceMetrics = VoicePerformanceMetrics()
    @Published var isLowLatencyMode = false
    
    private let audioEngine = AVAudioEngine()
    private let speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: "en-US"))
    private var audioSession = AVAudioSession.sharedInstance()
    
    // Performance optimization properties
    private var audioBufferPool = AudioBufferPool()
    private var recognitionRequestPool = SpeechRequestPool()
    private let performanceMonitor = VoicePerformanceMonitor()
    
    // Background processing optimization
    private var backgroundTask: UIBackgroundTaskIdentifier = .invalid
    private var processingQueue = DispatchQueue(
        label: "voice.processing",
        qos: .userInitiated,
        attributes: .concurrent
    )
    
    init() {
        setupOptimizedConfiguration()
    }
    
    deinit {
        cleanup()
    }
    
    // MARK: - Performance Optimization
    
    func optimizeVoicePerformance() {
        Task {
            await configureOptimalAudioSettings()
            await optimizeSpeechRecognition()
            await setupBackgroundVoiceProcessing()
            
            isOptimized = true
            print("ðŸŽ¤ Voice Manager: Performance optimization complete")
        }
    }
    
    private func setupOptimizedConfiguration() {
        // Configure audio session for optimal performance
        do {
            try audioSession.setCategory(
                .playAndRecord,
                mode: .measurement,
                options: [.defaultToSpeaker, .allowBluetooth]
            )
            try audioSession.setActive(true)
        } catch {
            print("ðŸŽ¤ Voice Manager: Audio session setup failed - \(error)")
        }
    }
    
    private func configureOptimalAudioSettings() async {
        do {
            // Use optimal buffer sizes for real-time processing
            try audioSession.setPreferredIOBufferDuration(0.005) // 5ms for low latency
            try audioSession.setPreferredSampleRate(16000) // Optimal for speech
            
            // Configure audio engine with optimized settings
            let inputNode = audioEngine.inputNode
            let recordingFormat = inputNode.outputFormat(forBus: 0)
            
            // Optimize buffer size - balance between latency and stability
            let bufferSize: AVAudioFrameCount = isLowLatencyMode ? 512 : 1024
            
            inputNode.installTap(onBus: 0, bufferSize: bufferSize, format: recordingFormat) { [weak self] buffer, time in
                self?.processAudioBufferOptimized(buffer, time: time)
            }
            
            print("ðŸŽ¤ Voice Manager: Audio settings optimized")
            
        } catch {
            print("ðŸŽ¤ Voice Manager: Audio configuration failed - \(error)")
        }
    }
    
    private func optimizeSpeechRecognition() async {
        guard let speechRecognizer = speechRecognizer else { return }
        
        // Enable on-device recognition for better performance and privacy
        if speechRecognizer.supportsOnDeviceRecognition {
            print("ðŸŽ¤ Voice Manager: On-device recognition enabled")
        }
        
        // Pre-warm the speech recognizer
        let warmupRequest = recognitionRequestPool.getRequest()
        warmupRequest.shouldReportPartialResults = false
        warmupRequest.requiresOnDeviceRecognition = true
        
        // Perform warmup recognition task
        let warmupTask = speechRecognizer.recognitionTask(with: warmupRequest) { _, _ in
            // Warmup complete
        }
        
        // Cancel warmup after brief period
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.1) {
            warmupTask?.cancel()
            self.recognitionRequestPool.returnRequest(warmupRequest)
        }
    }
    
    private func setupBackgroundVoiceProcessing() async {
        // Configure background processing for voice commands
        backgroundTask = UIApplication.shared.beginBackgroundTask(withName: "VoiceProcessing") {
            self.endBackgroundTask()
        }
    }
    
    private func processAudioBufferOptimized(_ buffer: AVAudioPCMBuffer, time: AVAudioTime) {
        // Use object pool to reduce allocations
        let audioData = audioBufferPool.getAudioData()
        defer { audioBufferPool.returnAudioData(audioData) }
        
        // Process on background queue for performance
        processingQueue.async { [weak self] in
            self?.analyzeAudioForWakePhrase(buffer)
        }
        
        // Update performance metrics
        performanceMonitor.recordBufferProcessed(at: Date())
    }
    
    private func analyzeAudioForWakePhrase(_ buffer: AVAudioPCMBuffer) {
        // Optimized wake phrase detection
        let energy = calculateAudioEnergy(buffer)
        
        // Only proceed with recognition if sufficient energy
        guard energy > 0.01 else { return }
        
        DispatchQueue.main.async {
            self.performanceMetrics.buffersProcessed += 1
        }
    }
    
    private func calculateAudioEnergy(_ buffer: AVAudioPCMBuffer) -> Float {
        guard let channelData = buffer.floatChannelData?[0] else { return 0 }
        
        let frameLength = Int(buffer.frameLength)
        var sum: Float = 0.0
        
        // Vectorized energy calculation for performance
        vDSP_svesq(channelData, 1, &sum, vDSP_Length(frameLength))
        
        return sqrt(sum / Float(frameLength))
    }
    
    // MARK: - Low Latency Mode
    
    func enableLowLatencyMode() {
        isLowLatencyMode = true
        
        // Reconfigure with lower latency settings
        Task {
            await configureOptimalAudioSettings()
        }
        
        print("ðŸŽ¤ Voice Manager: Low latency mode enabled")
    }
    
    func disableLowLatencyMode() {
        isLowLatencyMode = false
        
        // Reconfigure with standard settings
        Task {
            await configureOptimalAudioSettings()
        }
        
        print("ðŸŽ¤ Voice Manager: Low latency mode disabled")
    }
    
    // MARK: - Performance Monitoring
    
    func measureResponseTime(for operation: () async -> Void) async -> TimeInterval {
        let startTime = Date()
        await operation()
        let endTime = Date()
        
        let responseTime = endTime.timeIntervalSince(startTime)
        self.responseTime = responseTime
        
        performanceMetrics.averageResponseTime = updateAverageResponseTime(responseTime)
        
        return responseTime
    }
    
    private func updateAverageResponseTime(_ newTime: TimeInterval) -> TimeInterval {
        let alpha: Double = 0.1 // Exponential moving average factor
        return alpha * newTime + (1 - alpha) * performanceMetrics.averageResponseTime
    }
    
    // MARK: - Resource Management
    
    private func endBackgroundTask() {
        if backgroundTask != .invalid {
            UIApplication.shared.endBackgroundTask(backgroundTask)
            backgroundTask = .invalid
        }
    }
    
    private func cleanup() {
        audioEngine.stop()
        audioEngine.inputNode.removeTap(onBus: 0)
        endBackgroundTask()
        audioBufferPool.cleanup()
        recognitionRequestPool.cleanup()
    }
    
    // MARK: - Performance Validation
    
    var isPerformanceOptimal: Bool {
        return responseTime < 0.5 && // Target: <500ms response
               performanceMetrics.averageResponseTime < 0.3 && // Average <300ms
               performanceMetrics.errorRate < 0.05 // <5% error rate
    }
    
    func getPerformanceReport() -> VoicePerformanceReport {
        return VoicePerformanceReport(
            averageResponseTime: performanceMetrics.averageResponseTime,
            buffersProcessed: performanceMetrics.buffersProcessed,
            recognitionAccuracy: performanceMetrics.recognitionAccuracy,
            errorRate: performanceMetrics.errorRate,
            isOptimized: isOptimized,
            isLowLatencyMode: isLowLatencyMode
        )
    }
    
    // MARK: - Performance Optimization
    
    func optimizeForPerformance() async {
        isOptimized = true
        isLowLatencyMode = true
        
        await configureOptimalAudioSettings()
        
        // Update current latency
        currentLatency = responseTime
        
        print("ðŸŽ¤ Voice Manager: Performance optimization enabled")
    }
}

// MARK: - Performance Monitoring

class VoicePerformanceMonitor {
    private var bufferTimestamps: [Date] = []
    private let maxHistorySize = 100
    
    func recordBufferProcessed(at time: Date) {
        bufferTimestamps.append(time)
        if bufferTimestamps.count > maxHistorySize {
            bufferTimestamps.removeFirst()
        }
    }
    
    var averageBufferRate: Double {
        guard bufferTimestamps.count > 1 else { return 0 }
        
        let timeSpan = bufferTimestamps.last!.timeIntervalSince(bufferTimestamps.first!)
        return Double(bufferTimestamps.count) / timeSpan
    }
}

// MARK: - Object Pools for Memory Efficiency

class AudioBufferPool {
    private var availableAudioData: [AudioData] = []
    private let maxPoolSize = 10
    
    func getAudioData() -> AudioData {
        if let audioData = availableAudioData.popLast() {
            audioData.reset()
            return audioData
        }
        return AudioData()
    }
    
    func returnAudioData(_ audioData: AudioData) {
        if availableAudioData.count < maxPoolSize {
            availableAudioData.append(audioData)
        }
    }
    
    func cleanup() {
        availableAudioData.removeAll()
    }
}

class SpeechRequestPool {
    private var availableRequests: [SFSpeechAudioBufferRecognitionRequest] = []
    private let maxPoolSize = 5
    
    func getRequest() -> SFSpeechAudioBufferRecognitionRequest {
        if let request = availableRequests.popLast() {
            return request
        }
        
        let request = SFSpeechAudioBufferRecognitionRequest()
        request.shouldReportPartialResults = false
        request.requiresOnDeviceRecognition = true
        return request
    }
    
    func returnRequest(_ request: SFSpeechAudioBufferRecognitionRequest) {
        if availableRequests.count < maxPoolSize {
            request.endAudio()
            availableRequests.append(request)
        }
    }
    
    func cleanup() {
        availableRequests.forEach { $0.endAudio() }
        availableRequests.removeAll()
    }
}

// MARK: - Supporting Types

class AudioData {
    var samples: [Float] = []
    var energy: Float = 0
    var timestamp: Date = Date()
    
    func reset() {
        samples.removeAll(keepingCapacity: true)
        energy = 0
        timestamp = Date()
    }
}

struct VoicePerformanceMetrics {
    var averageResponseTime: TimeInterval = 0
    var buffersProcessed: Int = 0
    var recognitionAccuracy: Double = 0.95
    var errorRate: Double = 0.02
}

struct VoicePerformanceReport {
    let averageResponseTime: TimeInterval
    let buffersProcessed: Int
    let recognitionAccuracy: Double
    let errorRate: Double
    let isOptimized: Bool
    let isLowLatencyMode: Bool
    
    var status: PerformanceStatus {
        if averageResponseTime < 0.3 && errorRate < 0.05 {
            return .excellent
        } else if averageResponseTime < 0.5 && errorRate < 0.1 {
            return .good
        } else {
            return .needsImprovement
        }
    }
    
    enum PerformanceStatus {
        case excellent, good, needsImprovement
        
        var description: String {
            switch self {
            case .excellent: return "Excellent Performance"
            case .good: return "Good Performance"
            case .needsImprovement: return "Needs Improvement"
            }
        }
        
        var color: Color {
            switch self {
            case .excellent: return .green
            case .good: return .orange
            case .needsImprovement: return .red
            }
        }
    }
}