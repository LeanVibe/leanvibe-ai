"""
Centralized Error Tracking and Alerting System

Provides comprehensive error tracking with:
- Error categorization and severity levels
- Error rate monitoring and alerting
- Recovery metrics and suggestions
- Error aggregation and pattern detection
- Automated alert generation
- Integration with logging and monitoring systems
"""

import asyncio
import time
import traceback
from collections import defaultdict, deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum
from typing import Dict, List, Optional, Any, Callable, Set
import hashlib
import threading

from .logging_config import get_logger


logger = get_logger(__name__)


class ErrorSeverity(str, Enum):
    """Error severity levels"""
    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ErrorCategory(str, Enum):
    """Error categories for classification"""
    AUTHENTICATION = "authentication"
    AUTHORIZATION = "authorization"
    VALIDATION = "validation"
    DATABASE = "database"
    NETWORK = "network"
    EXTERNAL_SERVICE = "external_service"
    PERFORMANCE = "performance"
    RESOURCE = "resource"
    BUSINESS_LOGIC = "business_logic"
    SYSTEM = "system"
    UNKNOWN = "unknown"


class AlertType(str, Enum):
    """Types of alerts"""
    ERROR_RATE_SPIKE = "error_rate_spike"
    REPEATED_ERROR = "repeated_error"
    CRITICAL_ERROR = "critical_error"
    SERVICE_DEGRADATION = "service_degradation"
    RECOVERY_SUCCESS = "recovery_success"


@dataclass
class ErrorEvent:
    """Represents a single error event"""
    id: str
    timestamp: datetime
    error_type: str
    error_message: str
    severity: ErrorSeverity
    category: ErrorCategory
    service: str
    component: str
    stacktrace: Optional[str] = None
    context: Dict[str, Any] = field(default_factory=dict)
    request_id: Optional[str] = None
    user_id: Optional[str] = None
    resolved: bool = False
    resolution_time: Optional[datetime] = None
    recovery_actions: List[str] = field(default_factory=list)


@dataclass
class ErrorPattern:
    """Represents a pattern of recurring errors"""
    pattern_hash: str
    error_type: str
    error_message_template: str
    occurrences: int = 1
    first_seen: Optional[datetime] = None
    last_seen: Optional[datetime] = None
    affected_services: Set[str] = field(default_factory=set)
    affected_users: Set[str] = field(default_factory=set)
    severity: ErrorSeverity = ErrorSeverity.LOW
    category: ErrorCategory = ErrorCategory.UNKNOWN


@dataclass
class Alert:
    """Represents an alert generated by the error tracking system"""
    id: str
    alert_type: AlertType
    timestamp: datetime
    severity: ErrorSeverity
    title: str
    description: str
    affected_services: List[str]
    error_count: int
    time_window: str
    recommended_actions: List[str] = field(default_factory=list)
    acknowledged: bool = False
    acknowledged_by: Optional[str] = None
    acknowledged_at: Optional[datetime] = None
    resolved: bool = False
    resolved_at: Optional[datetime] = None


@dataclass
class ErrorMetrics:
    """Error metrics for a specific time period"""
    time_period: str
    total_errors: int
    errors_by_severity: Dict[str, int]
    errors_by_category: Dict[str, int]
    errors_by_service: Dict[str, int]
    error_rate_per_minute: float
    unique_error_types: int
    resolved_errors: int
    mean_resolution_time_minutes: Optional[float] = None


class ErrorTracker:
    """
    Centralized error tracking and alerting system
    
    Tracks all errors across the application, categorizes them,
    detects patterns, and generates appropriate alerts and recommendations.
    """
    
    def __init__(self, max_error_history: int = 10000):
        self.max_error_history = max_error_history
        self.error_history: deque = deque(maxlen=max_error_history)
        self.error_patterns: Dict[str, ErrorPattern] = {}
        self.active_alerts: Dict[str, Alert] = {}
        self.alert_history: deque = deque(maxlen=1000)
        
        # Error rate tracking
        self.error_rate_window = 300  # 5 minutes
        self.error_rate_buckets: deque = deque(maxlen=60)  # Last 60 5-second buckets
        
        # Alert thresholds
        self.alert_thresholds = {
            'error_rate_spike_factor': 3.0,  # 3x normal rate
            'error_rate_min_threshold': 5,   # Minimum 5 errors to trigger
            'repeated_error_threshold': 10,   # Same error 10 times
            'critical_error_immediate': True,  # Alert immediately on critical errors
            'time_window_minutes': 15,        # Time window for rate calculations
            'pattern_detection_min_occurrences': 3
        }
        
        # Recovery tracking
        self.recovery_metrics: Dict[str, List[float]] = defaultdict(list)
        
        # Background processing
        self._processing_active = False
        self._processing_task: Optional[asyncio.Task] = None
        
        # Thread safety
        self._lock = threading.RLock()
        
        # Error categorization rules
        self._categorization_rules = self._initialize_categorization_rules()
    
    def _initialize_categorization_rules(self) -> Dict[str, ErrorCategory]:
        """Initialize error categorization rules based on error types and messages"""
        return {
            # Authentication errors
            'AuthenticationError': ErrorCategory.AUTHENTICATION,
            'TokenExpiredError': ErrorCategory.AUTHENTICATION,
            'InvalidCredentialsError': ErrorCategory.AUTHENTICATION,
            'LoginFailedError': ErrorCategory.AUTHENTICATION,
            
            # Authorization errors
            'PermissionDeniedError': ErrorCategory.AUTHORIZATION,
            'ForbiddenError': ErrorCategory.AUTHORIZATION,
            'UnauthorizedError': ErrorCategory.AUTHORIZATION,
            
            # Validation errors
            'ValidationError': ErrorCategory.VALIDATION,
            'ValueError': ErrorCategory.VALIDATION,
            'InvalidInputError': ErrorCategory.VALIDATION,
            
            # Database errors
            'DatabaseError': ErrorCategory.DATABASE,
            'ConnectionError': ErrorCategory.DATABASE,
            'SQLError': ErrorCategory.DATABASE,
            'TransactionError': ErrorCategory.DATABASE,
            
            # Network errors
            'NetworkError': ErrorCategory.NETWORK,
            'TimeoutError': ErrorCategory.NETWORK,
            'ConnectionTimeoutError': ErrorCategory.NETWORK,
            'HTTPError': ErrorCategory.NETWORK,
            
            # External service errors
            'ExternalServiceError': ErrorCategory.EXTERNAL_SERVICE,
            'APIError': ErrorCategory.EXTERNAL_SERVICE,
            'ServiceUnavailableError': ErrorCategory.EXTERNAL_SERVICE,
            
            # Performance errors
            'PerformanceError': ErrorCategory.PERFORMANCE,
            'SlowQueryError': ErrorCategory.PERFORMANCE,
            'MemoryError': ErrorCategory.RESOURCE,
            'DiskSpaceError': ErrorCategory.RESOURCE,
            
            # System errors
            'SystemError': ErrorCategory.SYSTEM,
            'RuntimeError': ErrorCategory.SYSTEM,
            'OSError': ErrorCategory.SYSTEM,
        }
    
    def start_processing(self):
        """Start background error processing"""
        if self._processing_active:
            logger.warning("Error processing already active")
            return
        
        self._processing_active = True
        self._processing_task = asyncio.create_task(self._background_processing())
        logger.info("Error tracking background processing started")
    
    async def stop_processing(self):
        """Stop background error processing"""
        if not self._processing_active:
            return
        
        self._processing_active = False
        
        if self._processing_task:
            self._processing_task.cancel()
            try:
                await self._processing_task
            except asyncio.CancelledError:
                pass
        
        logger.info("Error tracking background processing stopped")
    
    async def _background_processing(self):
        """Background task for error pattern detection and alerting"""
        try:
            while self._processing_active:
                await self._process_error_patterns()
                await self._check_error_rates()
                await self._cleanup_old_data()
                await asyncio.sleep(30)  # Process every 30 seconds
        except asyncio.CancelledError:
            logger.info("Error processing background task cancelled")
        except Exception as e:
            logger.error("Error in background processing", error=str(e))
    
    def track_error(
        self,
        error: Exception,
        service: str,
        component: str,
        severity: Optional[ErrorSeverity] = None,
        category: Optional[ErrorCategory] = None,
        context: Optional[Dict[str, Any]] = None,
        request_id: Optional[str] = None,
        user_id: Optional[str] = None
    ) -> str:
        """
        Track an error event
        
        Args:
            error: The exception that occurred
            service: Service where the error occurred
            component: Component within the service
            severity: Error severity (auto-detected if not provided)
            category: Error category (auto-detected if not provided)
            context: Additional context information
            request_id: Request ID if available
            user_id: User ID if available
        
        Returns:
            Error ID for tracking
        """
        # Generate error ID
        error_id = self._generate_error_id(error, service, component)
        
        # Determine severity if not provided
        if severity is None:
            severity = self._determine_severity(error, service, component)
        
        # Determine category if not provided
        if category is None:
            category = self._categorize_error(error)
        
        # Extract stacktrace
        stacktrace = ''.join(traceback.format_exception(type(error), error, error.__traceback__))
        
        # Create error event
        error_event = ErrorEvent(
            id=error_id,
            timestamp=datetime.now(),
            error_type=type(error).__name__,
            error_message=str(error),
            severity=severity,
            category=category,
            service=service,
            component=component,
            stacktrace=stacktrace,
            context=context or {},
            request_id=request_id,
            user_id=user_id
        )
        
        # Store error
        with self._lock:
            self.error_history.append(error_event)
        
        # Update error patterns
        self._update_error_patterns(error_event)
        
        # Check for immediate alerts
        if severity == ErrorSeverity.CRITICAL or self.alert_thresholds['critical_error_immediate']:
            asyncio.create_task(self._generate_immediate_alert(error_event))
        
        # Log error
        log_func = logger.critical if severity == ErrorSeverity.CRITICAL else logger.error
        log_func(
            "Error tracked",
            error_id=error_id,
            error_type=error_event.error_type,
            service=service,
            component=component,
            severity=severity.value,
            category=category.value,
            request_id=request_id,
            user_id=user_id
        )
        
        return error_id
    
    def mark_error_resolved(self, error_id: str, recovery_actions: Optional[List[str]] = None):
        """Mark an error as resolved"""
        with self._lock:
            for error in self.error_history:
                if error.id == error_id:
                    error.resolved = True
                    error.resolution_time = datetime.now()
                    error.recovery_actions = recovery_actions or []
                    
                    # Track recovery metrics
                    resolution_time_minutes = (error.resolution_time - error.timestamp).total_seconds() / 60
                    service_key = f"{error.service}:{error.component}"
                    self.recovery_metrics[service_key].append(resolution_time_minutes)
                    
                    # Keep only last 100 recovery times per service
                    if len(self.recovery_metrics[service_key]) > 100:
                        self.recovery_metrics[service_key] = self.recovery_metrics[service_key][-100:]
                    
                    logger.info(
                        "Error marked as resolved",
                        error_id=error_id,
                        resolution_time_minutes=round(resolution_time_minutes, 2),
                        recovery_actions=recovery_actions
                    )
                    return
            
            logger.warning("Error not found for resolution", error_id=error_id)
    
    def _generate_error_id(self, error: Exception, service: str, component: str) -> str:
        """Generate a unique error ID"""
        timestamp_str = str(int(time.time() * 1000000))
        error_str = f"{service}:{component}:{type(error).__name__}:{str(error)[:100]}"
        hash_suffix = hashlib.md5(error_str.encode()).hexdigest()[:8]
        return f"err_{timestamp_str}_{hash_suffix}"
    
    def _determine_severity(self, error: Exception, service: str, component: str) -> ErrorSeverity:
        """Automatically determine error severity"""
        error_type = type(error).__name__
        error_message = str(error).lower()
        
        # Critical errors
        if any(keyword in error_type.lower() for keyword in ['critical', 'fatal', 'security']):
            return ErrorSeverity.CRITICAL
        
        if any(keyword in error_message for keyword in ['critical', 'fatal', 'security breach', 'data loss']):
            return ErrorSeverity.CRITICAL
        
        # High severity errors
        if error_type in ['DatabaseError', 'ConnectionError', 'AuthenticationError']:
            return ErrorSeverity.HIGH
        
        if any(keyword in error_message for keyword in ['timeout', 'unavailable', 'connection', 'database']):
            return ErrorSeverity.HIGH
        
        # Medium severity errors
        if error_type in ['ValidationError', 'PermissionDeniedError', 'HTTPError']:
            return ErrorSeverity.MEDIUM
        
        # Default to low severity
        return ErrorSeverity.LOW
    
    def _categorize_error(self, error: Exception) -> ErrorCategory:
        """Automatically categorize error based on type and message"""
        error_type = type(error).__name__
        error_message = str(error).lower()
        
        # Check direct mapping first
        if error_type in self._categorization_rules:
            return self._categorization_rules[error_type]
        
        # Check message keywords
        if any(keyword in error_message for keyword in ['auth', 'login', 'token', 'credential']):
            return ErrorCategory.AUTHENTICATION
        
        if any(keyword in error_message for keyword in ['permission', 'forbidden', 'unauthorized']):
            return ErrorCategory.AUTHORIZATION
        
        if any(keyword in error_message for keyword in ['validation', 'invalid', 'bad request']):
            return ErrorCategory.VALIDATION
        
        if any(keyword in error_message for keyword in ['database', 'sql', 'connection']):
            return ErrorCategory.DATABASE
        
        if any(keyword in error_message for keyword in ['network', 'timeout', 'connection']):
            return ErrorCategory.NETWORK
        
        if any(keyword in error_message for keyword in ['memory', 'disk', 'resource']):
            return ErrorCategory.RESOURCE
        
        return ErrorCategory.UNKNOWN
    
    def _update_error_patterns(self, error_event: ErrorEvent):
        """Update error patterns for pattern detection"""
        # Create pattern hash based on error type, service, and normalized message
        pattern_key = f"{error_event.error_type}:{error_event.service}:{error_event.component}"
        
        # Normalize error message for pattern matching
        normalized_message = self._normalize_error_message(error_event.error_message)
        pattern_hash = hashlib.md5(f"{pattern_key}:{normalized_message}".encode()).hexdigest()
        
        with self._lock:
            if pattern_hash in self.error_patterns:
                pattern = self.error_patterns[pattern_hash]
                pattern.occurrences += 1
                pattern.last_seen = error_event.timestamp
                pattern.affected_services.add(error_event.service)
                if error_event.user_id:
                    pattern.affected_users.add(error_event.user_id)
                
                # Update severity to highest seen
                if error_event.severity.value > pattern.severity.value:
                    pattern.severity = error_event.severity
                    
            else:
                self.error_patterns[pattern_hash] = ErrorPattern(
                    pattern_hash=pattern_hash,
                    error_type=error_event.error_type,
                    error_message_template=normalized_message,
                    first_seen=error_event.timestamp,
                    last_seen=error_event.timestamp,
                    affected_services={error_event.service},
                    affected_users={error_event.user_id} if error_event.user_id else set(),
                    severity=error_event.severity,
                    category=error_event.category
                )
    
    def _normalize_error_message(self, message: str) -> str:
        """Normalize error message for pattern detection"""
        # Remove specific values like IDs, timestamps, URLs
        import re
        normalized = message
        
        # Replace UUIDs
        normalized = re.sub(r'[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}', '<UUID>', normalized, flags=re.IGNORECASE)
        
        # Replace numbers
        normalized = re.sub(r'\b\d+\b', '<NUMBER>', normalized)
        
        # Replace file paths
        normalized = re.sub(r'\/[^\s]+', '<PATH>', normalized)
        
        # Replace URLs
        normalized = re.sub(r'https?://[^\s]+', '<URL>', normalized)
        
        # Replace quoted strings
        normalized = re.sub(r"'[^']*'", '<STRING>', normalized)
        normalized = re.sub(r'"[^"]*"', '<STRING>', normalized)
        
        return normalized
    
    async def _process_error_patterns(self):
        """Process error patterns to detect recurring issues"""
        with self._lock:
            patterns = dict(self.error_patterns)
        
        for pattern_hash, pattern in patterns.items():
            # Check for repeated error alerts
            if (pattern.occurrences >= self.alert_thresholds['repeated_error_threshold'] and
                pattern.occurrences % self.alert_thresholds['repeated_error_threshold'] == 0):
                
                await self._generate_pattern_alert(pattern)
    
    async def _check_error_rates(self):
        """Check for error rate spikes"""
        current_time = datetime.now()
        time_window = timedelta(minutes=self.alert_thresholds['time_window_minutes'])
        
        # Get errors in the time window
        with self._lock:
            recent_errors = [
                error for error in self.error_history
                if current_time - error.timestamp <= time_window
            ]
        
        if len(recent_errors) < self.alert_thresholds['error_rate_min_threshold']:
            return
        
        # Calculate current error rate
        current_rate = len(recent_errors) / self.alert_thresholds['time_window_minutes']
        
        # Get historical average (previous time window)
        historical_start = current_time - (time_window * 2)
        historical_end = current_time - time_window
        
        with self._lock:
            historical_errors = [
                error for error in self.error_history
                if historical_start <= current_time - error.timestamp <= historical_end
            ]
        
        if not historical_errors:
            return
        
        historical_rate = len(historical_errors) / self.alert_thresholds['time_window_minutes']
        
        # Check for spike
        if (current_rate > historical_rate * self.alert_thresholds['error_rate_spike_factor'] and
            current_rate > self.alert_thresholds['error_rate_min_threshold'] / self.alert_thresholds['time_window_minutes']):
            
            await self._generate_rate_spike_alert(current_rate, historical_rate, recent_errors)
    
    async def _generate_immediate_alert(self, error_event: ErrorEvent):
        """Generate immediate alert for critical errors"""
        alert_id = f"alert_{int(time.time() * 1000000)}"
        
        alert = Alert(
            id=alert_id,
            alert_type=AlertType.CRITICAL_ERROR,
            timestamp=datetime.now(),
            severity=error_event.severity,
            title=f"Critical Error in {error_event.service}",
            description=f"Critical error occurred: {error_event.error_type} - {error_event.error_message}",
            affected_services=[error_event.service],
            error_count=1,
            time_window="immediate",
            recommended_actions=self._get_recovery_recommendations(error_event)
        )
        
        with self._lock:
            self.active_alerts[alert_id] = alert
            self.alert_history.append(alert)
        
        logger.critical(
            "Critical error alert generated",
            alert_id=alert_id,
            error_type=error_event.error_type,
            service=error_event.service
        )
    
    async def _generate_pattern_alert(self, pattern: ErrorPattern):
        """Generate alert for repeated error pattern"""
        alert_id = f"alert_pattern_{pattern.pattern_hash}_{int(time.time())}"
        
        alert = Alert(
            id=alert_id,
            alert_type=AlertType.REPEATED_ERROR,
            timestamp=datetime.now(),
            severity=pattern.severity,
            title=f"Repeated Error Pattern: {pattern.error_type}",
            description=f"Error pattern '{pattern.error_type}' has occurred {pattern.occurrences} times",
            affected_services=list(pattern.affected_services),
            error_count=pattern.occurrences,
            time_window=f"since {pattern.first_seen.isoformat()}",
            recommended_actions=self._get_pattern_recovery_recommendations(pattern)
        )
        
        with self._lock:
            self.active_alerts[alert_id] = alert
            self.alert_history.append(alert)
        
        logger.warning(
            "Repeated error pattern alert generated",
            alert_id=alert_id,
            pattern_occurrences=pattern.occurrences,
            error_type=pattern.error_type
        )
    
    async def _generate_rate_spike_alert(self, current_rate: float, historical_rate: float, recent_errors: List[ErrorEvent]):
        """Generate alert for error rate spike"""
        alert_id = f"alert_rate_spike_{int(time.time())}"
        
        affected_services = list(set(error.service for error in recent_errors))
        
        alert = Alert(
            id=alert_id,
            alert_type=AlertType.ERROR_RATE_SPIKE,
            timestamp=datetime.now(),
            severity=ErrorSeverity.HIGH,
            title="Error Rate Spike Detected",
            description=f"Error rate increased from {historical_rate:.1f}/min to {current_rate:.1f}/min ({current_rate/historical_rate:.1f}x increase)",
            affected_services=affected_services,
            error_count=len(recent_errors),
            time_window=f"{self.alert_thresholds['time_window_minutes']} minutes",
            recommended_actions=[
                "Check system resources and performance",
                "Review recent deployments or configuration changes",
                "Monitor affected services for degradation",
                "Consider enabling circuit breakers or rate limiting"
            ]
        )
        
        with self._lock:
            self.active_alerts[alert_id] = alert
            self.alert_history.append(alert)
        
        logger.error(
            "Error rate spike alert generated",
            alert_id=alert_id,
            current_rate=current_rate,
            historical_rate=historical_rate,
            spike_factor=current_rate/historical_rate
        )
    
    def _get_recovery_recommendations(self, error_event: ErrorEvent) -> List[str]:
        """Get recovery recommendations based on error category"""
        recommendations = []
        
        if error_event.category == ErrorCategory.DATABASE:
            recommendations.extend([
                "Check database connection and health",
                "Verify database server resources",
                "Review recent database changes",
                "Consider connection pool adjustment"
            ])
        elif error_event.category == ErrorCategory.NETWORK:
            recommendations.extend([
                "Check network connectivity",
                "Verify external service availability",
                "Review firewall and security group settings",
                "Consider implementing retry logic with backoff"
            ])
        elif error_event.category == ErrorCategory.AUTHENTICATION:
            recommendations.extend([
                "Check authentication service status",
                "Verify token/certificate validity",
                "Review authentication configuration",
                "Monitor for security incidents"
            ])
        elif error_event.category == ErrorCategory.RESOURCE:
            recommendations.extend([
                "Check system resource usage (CPU, memory, disk)",
                "Scale resources if needed",
                "Review resource allocation and limits",
                "Investigate memory leaks or resource contention"
            ])
        else:
            recommendations.extend([
                "Review application logs for additional context",
                "Check service dependencies",
                "Verify configuration settings",
                "Consider rolling back recent changes"
            ])
        
        return recommendations
    
    def _get_pattern_recovery_recommendations(self, pattern: ErrorPattern) -> List[str]:
        """Get recovery recommendations for error patterns"""
        base_recommendations = self._get_recovery_recommendations(
            type('ErrorEvent', (), {
                'category': pattern.category,
                'service': list(pattern.affected_services)[0] if pattern.affected_services else 'unknown'
            })()
        )
        
        pattern_specific = [
            f"Investigate root cause of recurring {pattern.error_type}",
            "Consider implementing circuit breaker pattern",
            "Review error handling and retry logic",
            f"Pattern affects {len(pattern.affected_services)} services and {len(pattern.affected_users)} users"
        ]
        
        return base_recommendations + pattern_specific
    
    async def _cleanup_old_data(self):
        """Clean up old alerts and patterns"""
        cutoff_time = datetime.now() - timedelta(hours=24)
        
        with self._lock:
            # Remove old resolved alerts
            expired_alerts = [
                alert_id for alert_id, alert in self.active_alerts.items()
                if alert.resolved and alert.resolved_at and alert.resolved_at < cutoff_time
            ]
            
            for alert_id in expired_alerts:
                del self.active_alerts[alert_id]
            
            # Clean up old error patterns (keep only active ones)
            old_patterns = [
                pattern_hash for pattern_hash, pattern in self.error_patterns.items()
                if pattern.last_seen and pattern.last_seen < cutoff_time
            ]
            
            for pattern_hash in old_patterns:
                del self.error_patterns[pattern_hash]
    
    def get_error_metrics(self, time_window_hours: int = 24) -> ErrorMetrics:
        """Get error metrics for a specific time window"""
        cutoff_time = datetime.now() - timedelta(hours=time_window_hours)
        
        with self._lock:
            errors_in_window = [
                error for error in self.error_history
                if error.timestamp >= cutoff_time
            ]
        
        if not errors_in_window:
            return ErrorMetrics(
                time_period=f"{time_window_hours}h",
                total_errors=0,
                errors_by_severity={},
                errors_by_category={},
                errors_by_service={},
                error_rate_per_minute=0.0,
                unique_error_types=0,
                resolved_errors=0
            )
        
        # Calculate metrics
        errors_by_severity = defaultdict(int)
        errors_by_category = defaultdict(int)
        errors_by_service = defaultdict(int)
        
        resolved_errors = 0
        resolution_times = []
        
        for error in errors_in_window:
            errors_by_severity[error.severity.value] += 1
            errors_by_category[error.category.value] += 1
            errors_by_service[error.service] += 1
            
            if error.resolved and error.resolution_time:
                resolved_errors += 1
                resolution_time_minutes = (error.resolution_time - error.timestamp).total_seconds() / 60
                resolution_times.append(resolution_time_minutes)
        
        unique_error_types = len(set(error.error_type for error in errors_in_window))
        error_rate_per_minute = len(errors_in_window) / (time_window_hours * 60)
        mean_resolution_time = sum(resolution_times) / len(resolution_times) if resolution_times else None
        
        return ErrorMetrics(
            time_period=f"{time_window_hours}h",
            total_errors=len(errors_in_window),
            errors_by_severity=dict(errors_by_severity),
            errors_by_category=dict(errors_by_category),
            errors_by_service=dict(errors_by_service),
            error_rate_per_minute=round(error_rate_per_minute, 2),
            unique_error_types=unique_error_types,
            resolved_errors=resolved_errors,
            mean_resolution_time_minutes=round(mean_resolution_time, 2) if mean_resolution_time else None
        )
    
    def get_active_alerts(self) -> List[Alert]:
        """Get all active alerts"""
        with self._lock:
            return [alert for alert in self.active_alerts.values() if not alert.resolved]
    
    def acknowledge_alert(self, alert_id: str, acknowledged_by: str):
        """Acknowledge an alert"""
        with self._lock:
            if alert_id in self.active_alerts:
                alert = self.active_alerts[alert_id]
                alert.acknowledged = True
                alert.acknowledged_by = acknowledged_by
                alert.acknowledged_at = datetime.now()
                
                logger.info("Alert acknowledged", alert_id=alert_id, acknowledged_by=acknowledged_by)
            else:
                logger.warning("Alert not found for acknowledgment", alert_id=alert_id)
    
    def resolve_alert(self, alert_id: str):
        """Resolve an alert"""
        with self._lock:
            if alert_id in self.active_alerts:
                alert = self.active_alerts[alert_id]
                alert.resolved = True
                alert.resolved_at = datetime.now()
                
                logger.info("Alert resolved", alert_id=alert_id)
            else:
                logger.warning("Alert not found for resolution", alert_id=alert_id)
    
    def get_error_patterns(self, min_occurrences: int = 1) -> List[ErrorPattern]:
        """Get error patterns with minimum occurrence count"""
        with self._lock:
            return [
                pattern for pattern in self.error_patterns.values()
                if pattern.occurrences >= min_occurrences
            ]
    
    def get_recovery_metrics(self) -> Dict[str, Any]:
        """Get error recovery metrics"""
        with self._lock:
            recovery_data = dict(self.recovery_metrics)
        
        service_recovery_stats = {}
        
        for service_key, resolution_times in recovery_data.items():
            if resolution_times:
                service_recovery_stats[service_key] = {
                    'total_recoveries': len(resolution_times),
                    'avg_resolution_time_minutes': round(sum(resolution_times) / len(resolution_times), 2),
                    'min_resolution_time_minutes': round(min(resolution_times), 2),
                    'max_resolution_time_minutes': round(max(resolution_times), 2),
                    'recent_recovery_trend': 'improving' if len(resolution_times) >= 5 and 
                                          sum(resolution_times[-3:]) / 3 < sum(resolution_times[-6:-3]) / 3 
                                          else 'stable'
                }
        
        return {
            'service_recovery_stats': service_recovery_stats,
            'total_tracked_services': len(service_recovery_stats),
            'overall_recovery_rate': sum(
                stats['total_recoveries'] for stats in service_recovery_stats.values()
            )
        }


# Global error tracker instance
error_tracker = ErrorTracker()


# Convenience functions
def track_error(
    error: Exception,
    service: str,
    component: str,
    severity: Optional[ErrorSeverity] = None,
    category: Optional[ErrorCategory] = None,
    **kwargs
) -> str:
    """Track an error (convenience function)"""
    return error_tracker.track_error(
        error=error,
        service=service,
        component=component,
        severity=severity,
        category=category,
        **kwargs
    )


def mark_error_resolved(error_id: str, recovery_actions: Optional[List[str]] = None):
    """Mark an error as resolved"""
    error_tracker.mark_error_resolved(error_id, recovery_actions)


def get_error_summary() -> Dict[str, Any]:
    """Get comprehensive error tracking summary"""
    metrics_24h = error_tracker.get_error_metrics(24)
    active_alerts = error_tracker.get_active_alerts()
    error_patterns = error_tracker.get_error_patterns(min_occurrences=3)
    recovery_metrics = error_tracker.get_recovery_metrics()
    
    return {
        'metrics_24h': {
            'total_errors': metrics_24h.total_errors,
            'error_rate_per_minute': metrics_24h.error_rate_per_minute,
            'resolved_errors': metrics_24h.resolved_errors,
            'resolution_rate': metrics_24h.resolved_errors / metrics_24h.total_errors if metrics_24h.total_errors > 0 else 0,
            'errors_by_severity': metrics_24h.errors_by_severity,
            'errors_by_category': metrics_24h.errors_by_category
        },
        'active_alerts': len(active_alerts),
        'unacknowledged_alerts': len([a for a in active_alerts if not a.acknowledged]),
        'error_patterns': len(error_patterns),
        'recovery_metrics': recovery_metrics
    }


async def start_error_tracking():
    """Start error tracking background processing"""
    error_tracker.start_processing()


async def stop_error_tracking():
    """Stop error tracking background processing"""
    await error_tracker.stop_processing()