name: Weekly Comprehensive Validation
# Tier 3 tests: Full mutation testing, dependency updates, comprehensive load testing
# Runs weekly for complete system validation

on:
  schedule:
    # Run every Sunday at 3 AM UTC
    - cron: '0 3 * * 0'
  workflow_dispatch:
    inputs:
      full_mutation:
        description: 'Run full mutation testing (100%)'
        required: false
        default: 'true'
      dependency_update:
        description: 'Perform dependency updates'
        required: false
        default: 'true'
      stress_test_duration:
        description: 'Stress test duration (minutes)'
        required: false
        default: '60'

env:
  PYTHON_VERSION: "3.11"
  UV_CACHE_DIR: /tmp/.uv-cache

jobs:
  # Dependency Security Audit and Updates
  dependency-maintenance:
    name: "Dependency Updates & Security"
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: github.event.inputs.dependency_update != 'false'
    
    steps:
    - uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
    
    - name: Install dependencies
      run: |
        uv sync --frozen
        uv add pip-audit renovate-bot safety
    
    - name: Comprehensive Security Audit
      run: |
        mkdir -p test_results/security/
        # Multiple security scanning tools
        uv run pip-audit --format=json --output=test_results/security/pip_audit.json || true
        uv run safety check --json --output=test_results/security/safety_full.json || true
        
        # Custom dependency analysis
        uv run python scripts/validate_all_dependencies.py \
          --output=test_results/security/dependency_analysis.json
    
    - name: Check for Dependency Updates
      run: |
        # Check for available updates
        uv lock --upgrade-package=* --dry-run > test_results/available_updates.txt || true
        
        # Generate update recommendations
        uv run python tools/dependency_analyzer.py \
          --current-lock=uv.lock \
          --security-reports=test_results/security/ \
          --output=test_results/update_recommendations.json
    
    - name: Apply Critical Security Updates
      run: |
        # Only apply critical security updates automatically
        if [ -f test_results/update_recommendations.json ]; then
          uv run python tools/dependency_analyzer.py apply-critical \
            --recommendations=test_results/update_recommendations.json \
            --dry-run=false
        fi
    
    - name: Test Updated Dependencies
      run: |
        uv sync
        # Run basic test suite to ensure updates don't break functionality
        uv run pytest \
          -m "unit and not slow" \
          --maxfail=10 \
          --tb=short
    
    - name: Create Dependency Update PR
      if: success()
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "chore: automated dependency security updates"
        title: "🔒 Weekly Dependency Security Updates"
        body: |
          ## Automated Dependency Security Updates
          
          This PR contains automated security updates for dependencies.
          
          ### Security Analysis
          - Critical vulnerabilities: See attached reports
          - Updates applied: See `test_results/update_recommendations.json`
          
          ### Testing
          - ✅ Basic test suite passed
          - ✅ Security scans completed
          
          **Auto-generated by weekly maintenance workflow**
        branch: dependency-updates/weekly-${{ github.run_number }}
        labels: |
          security
          dependencies
          auto-merge
    
    - name: Upload Security Results
      uses: actions/upload-artifact@v3
      with:
        name: dependency-security-results
        path: test_results/

  # Full Mutation Testing
  full-mutation-testing:
    name: "Full Mutation Testing"
    runs-on: ubuntu-latest
    timeout-minutes: 120
    if: github.event.inputs.full_mutation != 'false'
    
    strategy:
      matrix:
        module: [api, services, core, models, utils]
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
    
    - name: Install dependencies
      run: |
        uv sync --frozen
        uv add mutmut
    
    - name: Setup test environment
      run: |
        docker compose -f docker-compose.test.yml up -d
        sleep 20
    
    - name: Run Full Mutation Testing - ${{ matrix.module }}
      run: |
        # Run complete mutation testing for the module
        uv run mutmut run \
          --paths-to-mutate=app/${{ matrix.module }} \
          --tests-dir=tests \
          --runner='python -m pytest -m "unit or integration" --tb=no -q --maxfail=50' \
          --use-coverage \
          --percentage-to-mutate=100
    
    - name: Generate Detailed Mutation Report
      run: |
        mkdir -p test_results/mutation/
        uv run mutmut results > test_results/mutation/full_${{ matrix.module }}.txt
        uv run mutmut html --directory test_results/mutation/html_${{ matrix.module }}
        uv run mutmut json > test_results/mutation/full_${{ matrix.module }}.json
    
    - name: Calculate Mutation Score
      run: |
        SCORE=$(uv run mutmut results | grep -o "mutation score [0-9.]*%" | cut -d' ' -f3)
        echo "Mutation Score for ${{ matrix.module }}: $SCORE" >> $GITHUB_STEP_SUMMARY
        echo "{\"module\": \"${{ matrix.module }}\", \"score\": \"$SCORE\", \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"}" > test_results/mutation/score_${{ matrix.module }}.json
    
    - name: Upload Mutation Results
      uses: actions/upload-artifact@v3
      with:
        name: full-mutation-results-${{ matrix.module }}
        path: test_results/mutation/

  # Comprehensive Load and Stress Testing
  stress-testing:
    name: "Comprehensive Load Testing"
    runs-on: ubuntu-latest
    timeout-minutes: 90
    
    strategy:
      matrix:
        scenario: [baseline, spike, soak, breakpoint]
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
    
    - name: Install dependencies
      run: |
        uv sync --frozen
        uv add locust k6 artillery
    
    - name: Setup production-like environment
      run: |
        # Use production configuration for load testing
        docker compose -f docker-compose.test.yml -f docker-compose.prod-like.yml up -d
        sleep 30
        
        # Start application with production settings
        export ENVIRONMENT=production
        uv run uvicorn app.main:app --host 0.0.0.0 --port 8000 --workers 4 &
        sleep 15
    
    - name: Warm up application
      run: |
        # Warm up caches and connections
        for i in {1..10}; do
          curl -f http://localhost:8000/health/complete || true
          sleep 1
        done
    
    - name: Run Load Test - ${{ matrix.scenario }}
      run: |
        mkdir -p test_results/load/${{ matrix.scenario }}
        
        case "${{ matrix.scenario }}" in
          baseline)
            # Baseline: Normal expected load
            uv run locust -f tests/load_tests/comprehensive.py \
              --host=http://localhost:8000 \
              --users=50 --spawn-rate=5 --run-time=1800s \
              --html=test_results/load/${{ matrix.scenario }}/report.html \
              --csv=test_results/load/${{ matrix.scenario }}/results \
              --headless
            ;;
          spike)
            # Spike: Sudden load increases
            uv run locust -f tests/load_tests/spike.py \
              --host=http://localhost:8000 \
              --users=200 --spawn-rate=50 --run-time=600s \
              --html=test_results/load/${{ matrix.scenario }}/report.html \
              --csv=test_results/load/${{ matrix.scenario }}/results \
              --headless
            ;;
          soak)
            # Soak: Long-running steady load
            duration=${{ github.event.inputs.stress_test_duration || '60' }}
            uv run locust -f tests/load_tests/soak.py \
              --host=http://localhost:8000 \
              --users=30 --spawn-rate=2 --run-time=${duration}m \
              --html=test_results/load/${{ matrix.scenario }}/report.html \
              --csv=test_results/load/${{ matrix.scenario }}/results \
              --headless
            ;;
          breakpoint)
            # Breakpoint: Find system limits
            uv run locust -f tests/load_tests/breakpoint.py \
              --host=http://localhost:8000 \
              --users=500 --spawn-rate=10 --run-time=1200s \
              --html=test_results/load/${{ matrix.scenario }}/report.html \
              --csv=test_results/load/${{ matrix.scenario }}/results \
              --headless
            ;;
        esac
    
    - name: Analyze Load Test Results
      run: |
        uv run python tools/load_analyzer.py \
          --scenario=${{ matrix.scenario }} \
          --results-dir=test_results/load/${{ matrix.scenario }} \
          --output=test_results/load/${{ matrix.scenario }}/analysis.json
    
    - name: Check Performance SLA
      run: |
        uv run python tools/sla_checker.py \
          --scenario=${{ matrix.scenario }} \
          --analysis=test_results/load/${{ matrix.scenario }}/analysis.json \
          --thresholds=config/performance_sla.json
    
    - name: Upload Load Test Results
      uses: actions/upload-artifact@v3
      with:
        name: load-test-results-${{ matrix.scenario }}
        path: test_results/load/${{ matrix.scenario }}/

  # Chaos Engineering (Light)
  chaos-testing:
    name: "Chaos Engineering"
    runs-on: ubuntu-latest
    timeout-minutes: 45
    
    strategy:
      matrix:
        chaos_type: [network-partition, service-failure, resource-exhaustion]
      fail-fast: false
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      uses: astral-sh/setup-uv@v3
      with:
        enable-cache: true
    
    - name: Install dependencies
      run: |
        uv sync --frozen
        uv add chaos-toolkit chaostoolkit-lib
    
    - name: Setup resilient environment
      run: |
        docker compose -f docker-compose.test.yml -f docker-compose.chaos.yml up -d
        sleep 30
    
    - name: Run Chaos Experiment - ${{ matrix.chaos_type }}
      run: |
        mkdir -p test_results/chaos/${{ matrix.chaos_type }}
        
        uv run chaos run \
          tests/chaos_experiments/${{ matrix.chaos_type }}.json \
          --journal-path=test_results/chaos/${{ matrix.chaos_type }}/journal.json
    
    - name: Analyze Chaos Results
      run: |
        uv run python tools/chaos_analyzer.py \
          --journal=test_results/chaos/${{ matrix.chaos_type }}/journal.json \
          --output=test_results/chaos/${{ matrix.chaos_type }}/analysis.json
    
    - name: Upload Chaos Results
      uses: actions/upload-artifact@v3
      with:
        name: chaos-test-results-${{ matrix.chaos_type }}
        path: test_results/chaos/${{ matrix.chaos_type }}/

  # Database Migration Testing
  database-migration-testing:
    name: "Database Migration Testing"
    runs-on: ubuntu-latest
    timeout-minutes: 30
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install UV
      uses: astral-sh/setup-uv@v3
    
    - name: Install dependencies
      run: uv sync --frozen
    
    - name: Test Database Migrations
      run: |
        # Test migrations against different database versions
        for db_version in "latest" "previous" "oldest-supported"; do
          echo "Testing migration against $db_version"
          
          docker run -d --name neo4j-$db_version \
            -p 7474:7474 -p 7687:7687 \
            -e NEO4J_AUTH=neo4j/testpassword \
            neo4j:$db_version
          
          sleep 20
          
          # Run migration tests
          uv run pytest tests/migration_tests/ \
            -v --tb=short \
            --db-version=$db_version
          
          docker stop neo4j-$db_version
          docker rm neo4j-$db_version
        done
    
    - name: Schema Drift Detection
      run: |
        uv run python tools/schema_drift.py \
          --current-schema=app/models/ \
          --baseline-schema=tests/fixtures/baseline_schema.json \
          --output=test_results/schema_drift.json

  # Generate Weekly Quality Assessment
  weekly-assessment:
    name: "Weekly Quality Assessment"
    runs-on: ubuntu-latest
    needs: [dependency-maintenance, full-mutation-testing, stress-testing, chaos-testing, database-migration-testing]
    if: always()
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
      with:
        path: artifacts/
    
    - name: Install UV
      uses: astral-sh/setup-uv@v3
    
    - name: Install dependencies
      run: uv sync --frozen
    
    - name: Generate Comprehensive Weekly Report
      run: |
        uv run python tools/weekly_assessor.py \
          --artifacts-dir=artifacts/ \
          --output=weekly_quality_assessment.md \
          --json-output=test_results/weekly_metrics.json \
          --trends-file=test_results/metrics/weekly_trends.json \
          --date="$(date -u +%Y-%m-%d)"
    
    - name: Calculate Quality Score
      run: |
        QUALITY_SCORE=$(uv run python tools/quality_calculator.py \
          --weekly-metrics=test_results/weekly_metrics.json)
        
        echo "## 📊 Weekly Quality Score: $QUALITY_SCORE/100" >> $GITHUB_STEP_SUMMARY
        echo "QUALITY_SCORE=$QUALITY_SCORE" >> $GITHUB_ENV
    
    - name: Update Quality Dashboard
      run: |
        uv run python tools/dashboard_updater.py \
          --quality-score=$QUALITY_SCORE \
          --weekly-metrics=test_results/weekly_metrics.json \
          --dashboard-file=docs/quality_dashboard.md
    
    - name: Generate Executive Summary
      run: |
        echo "## 🏆 Weekly Executive Summary" >> $GITHUB_STEP_SUMMARY
        cat weekly_quality_assessment.md >> $GITHUB_STEP_SUMMARY
    
    - name: Archive Results
      uses: actions/upload-artifact@v3
      with:
        name: weekly-quality-assessment
        path: |
          weekly_quality_assessment.md
          test_results/weekly_metrics.json
          test_results/metrics/
        retention-days: 90
    
    - name: Create Quality Issue if Score < 80
      if: env.QUALITY_SCORE < 80
      uses: peter-evans/create-pull-request@v5
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        commit-message: "docs: weekly quality assessment - action required"
        title: "🚨 Quality Score Below Threshold (${{ env.QUALITY_SCORE }}/100)"
        body: |
          ## Quality Alert - Action Required
          
          The weekly quality assessment has identified areas requiring attention.
          
          **Current Score: ${{ env.QUALITY_SCORE }}/100** (Threshold: 80)
          
          ### Action Items
          See attached `weekly_quality_assessment.md` for detailed recommendations.
          
          ### Trend Analysis
          Quality trends and historical data available in artifacts.
          
          **This issue requires human review and action planning.**
        branch: quality-alert/week-${{ github.run_number }}
        labels: |
          quality
          urgent
          review-required

  # Cleanup and Maintenance
  cleanup-maintenance:
    name: "Cleanup & Maintenance"
    runs-on: ubuntu-latest
    needs: [dependency-maintenance, full-mutation-testing, stress-testing, chaos-testing, database-migration-testing, weekly-assessment]
    if: always()
    
    steps:
    - name: Cleanup Docker Resources
      run: |
        docker system prune -af --volumes
        docker network prune -f
    
    - name: Cleanup Old Test Results
      uses: actions/github-script@v6
      with:
        script: |
          const { repo, owner } = context.repo;
          const sixMonthsAgo = new Date();
          sixMonthsAgo.setMonth(sixMonthsAgo.getMonth() - 6);
          
          // Cleanup old artifacts (keep 6 months)
          const artifacts = await github.rest.actions.listArtifactsForRepo({
            owner,
            repo,
            per_page: 100
          });
          
          let cleanedCount = 0;
          for (const artifact of artifacts.data.artifacts) {
            if (new Date(artifact.created_at) < sixMonthsAgo) {
              await github.rest.actions.deleteArtifact({
                owner,
                repo,
                artifact_id: artifact.id
              });
              cleanedCount++;
            }
          }
          
          console.log(`Cleaned up ${cleanedCount} old artifacts`);
    
    - name: Generate Maintenance Summary
      run: |
        echo "## 🧹 Weekly Maintenance Summary" >> $GITHUB_STEP_SUMMARY
        echo "- Docker resources cleaned" >> $GITHUB_STEP_SUMMARY
        echo "- Old artifacts archived" >> $GITHUB_STEP_SUMMARY
        echo "- Quality assessment completed" >> $GITHUB_STEP_SUMMARY